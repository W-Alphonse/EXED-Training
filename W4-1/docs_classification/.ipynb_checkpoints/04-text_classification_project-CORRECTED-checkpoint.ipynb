{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification pipeline: Application with text data\n",
    "\n",
    "Author: Alexandre Gramfort\n",
    "\n",
    "The objective of this hands on session is to setup a predictive pipeline to classify movie critics. Critics are either positive (y=1) or negative (y=0). This task is often referred to as **sentiment analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you are being given :\n",
    "\n",
    "- critics of movies in text files in folder *data/imdb1*,\n",
    "\n",
    "Your mission :\n",
    "\n",
    "- Extract numeric features (the 'X') from the raw data (word counts)\n",
    "- Apply a Logistic Regression classifier with proper setup of hyperparameter\n",
    "- Evaluate performance in terms of accuracy with cross-validation\n",
    "- Answer the question \"Do I have enough data?\" using a learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's load the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 documents\n",
      "Number of positives 1000 and negatives 1000\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "# The glob module finds all the pathnames matching a specified pattern\n",
    "filenames_neg = sorted(glob(op.join('data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))\n",
    "print(\"Number of positives %s and negatives %s\" % (len(texts_pos), len(texts_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- What does the array `y` correspond to?\n",
    "- What is the type of the variables `texts`?\n",
    "- Can you read the first text?\n",
    "- Complete the function **count_words** that counts the number of occurences of each word in a list of texts. You'll need to use the *split* method from the string class to split a text in words.\n",
    "\n",
    "Example of usage of the `split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'DSSP', 'attendees!']\n",
      "number of words : 3\n"
     ]
    }
   ],
   "source": [
    "words = \"Hello DSSP attendees!\".split()\n",
    "print(words)\n",
    "print(\"number of words : %s\" % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " \"what's\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '\"',\n",
       " 'sorta',\n",
       " '\"',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " \"didn't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " \"it's\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '\"',\n",
       " 'normal',\n",
       " '\"',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '\"',\n",
       " 'fantasy',\n",
       " '\"',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " \"what's\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " \"don't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " \"film's\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " \"it's\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " \"didn't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '\"',\n",
       " 'into',\n",
       " 'it',\n",
       " '\"',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " \"might've\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '\"',\n",
       " 'the',\n",
       " 'suits',\n",
       " '\"',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " \"character's\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " \"doesn't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " \"doesn't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " \"it's\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " \"it's\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " \"where's\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_text = texts[0]\n",
    "this_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of usage of the `count_words` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> some_texts = ['A B B', 'B', 'A A']\n",
    ">>> vocabulary, counts = count_words(some_texts)\n",
    ">>> print(vocabulary)  # dictionary word -> column index\n",
    "{'A': 0, 'B': 1}\n",
    ">>> print(counts)  # number of occurence of each word from vocabulary in each text\n",
    "[[ 1.  2.]\n",
    " [ 0.  1.]\n",
    " [ 2.  0.]]\n",
    "```\n",
    "\n",
    "**Remark:** The vocabularty is a `dict` and its values have nothing to do with a number of occurences. Its values are the indices of columns in the `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'A': 0, 'B': 1}, array([[1., 2.],\n",
       "        [0., 1.],\n",
       "        [2., 0.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    # Strat 1\n",
    "    vocabulary = dict()\n",
    "    n_words = 0\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = n_words\n",
    "                n_words += 1\n",
    "\n",
    "\n",
    "    counts = np.zeros((len(texts), n_words))\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        for word in text.split():\n",
    "            counts[i, vocabulary[word]] += 1\n",
    "\n",
    "    return vocabulary, counts\n",
    "\n",
    "some_texts = ['A B B', 'B', 'A A']\n",
    "count_words(some_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 50920)\n",
      "1492681.0\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocabulary, counts = count_words(texts)\n",
    "print(counts.shape)\n",
    "print(counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Estimate Logistic regression on the full data. Show the effect of overfitting by evaluating the predictive power of your method in terms of accuracy.\n",
    "- Use the `train_test_split` function split the data in train and test (80% train and 20% test). What performance do you get?\n",
    "- Can you do better by adjusting the regularization parameter C? Use values between 0.00001 and 1000.\n",
    "- Why is this potentially dangerous? How do you avoid troubles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e-05, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear', C=1e-5)\n",
    "\n",
    "clf.fit(counts, y)\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6215"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate Logistic regression on the full data. \n",
    "# Show the effect of overfitting by evaluating the predictive power of your method in terms of accuracy.\n",
    "np.mean(clf.predict(counts) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# HAA - Test LR classification using ManualBagOfWords + Bad Regularization\n",
    "#       => Score = 0.61\n",
    "# ------------------------------------------------------------------------\n",
    "# Use the train_test_split function split the data in train and test (80% train and 20% test).\n",
    "# What performance do you get?\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(counts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C : 1e-05 - Score : 0.61\n",
      "C : 0.0001 - Score : 0.725\n",
      "C : 0.001 - Score : 0.8075\n",
      "C : 0.01 - Score : 0.8525\n",
      "C : 0.1 - Score : 0.8475\n",
      "C : 1.0 - Score : 0.845\n",
      "C : 10.0 - Score : 0.835\n",
      "C : 100.0 - Score : 0.8275\n",
      "C : 1000.0 - Score : 0.83\n",
      "\n",
      "*** Best C : 0.01 - Best Score : 0.8525 ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Can you do better by adjusting the regularization parameter C? \n",
    "# Use values between 0.00001 (=> High Regul) and 1000(=> Low Regul)\n",
    "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 10., 100., 1000.]\n",
    "\n",
    "scores = []\n",
    "for C in Cs:\n",
    "    clf.C = C\n",
    "    scores.append(clf.fit(X_train, y_train).score(X_test, y_test))\n",
    "    print('C : %s - Score : %s' % (C, scores[-1]))\n",
    "\n",
    "C_best = Cs[np.argmax(scores)]\n",
    "print('\\n*** Best C : %s - Best Score : %s ***' % (C_best, np.max(scores)))\n",
    "\n",
    "plt.plot(np.log10(Cs), scores)\n",
    "plt.axvline(np.log10(C_best), color='k')\n",
    "plt.xlabel(\"log10(C)\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# HAA - Test LR classification using ManualBagOfWords + Right Regularization\n",
    "#       => Score = 0.64\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(counts, y, train_size=0.8, random_state=None)\n",
    "\n",
    "clf.C = 0.01\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Compare the performance of Logistic Regression vs Multinomial Nayes Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# TODO\n",
    "clf.fit(counts, y)\n",
    "np.mean(clf.predict(counts) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split arrays or matrices into random train and test subsets\n",
    "# --------------------------------------------------------------------------\n",
    "# HAA - Test MultinomialNB classification using ManualBagOfWords \n",
    "#       => Score = 0.82\n",
    "# ---------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(counts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Compare your implementation of word counting with scikit-learn.\n",
    "\n",
    "For this use the classes *CountVectorizer* and a *Pipeline*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score  # replace by cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# The lower and upper boundary of the range of n-values for different n-grams to be extracted. \n",
    "# All values of n such that min_n <= n <= max_n will be used\n",
    "# n-gram is basically set of occurring words within given window so when\n",
    "# n=1 it is Unigram\n",
    "# n=2 it is bigram\n",
    "# n=3 it is trigram and so on\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.75, ngram_range=(1, 2),  # max_df = When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "                             analyzer='word', stop_words=None)),\n",
    "    ('logreg', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# texts : the features of all samples. A list of texts of all positive and negative texts\n",
    "# y : the target. 1d array of 1 and 0 according to the criticism\n",
    "# clf.fit(texts, y)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# HAA - Test LR classification using CountVectorizer and without any Regularization\n",
    "#       => Code = 0.86\n",
    "# ---------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8775"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# HAA - Test LR classification using CountVectorizer + Regularization\n",
    "#       => Code = 0.8775\n",
    "# -------------------------------------------------------------------\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.75, ngram_range=(1, 2),  # max_df = When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "                             analyzer='word', stop_words=None)),\n",
    "    ('logreg', LogisticRegression(solver='liblinear',  C=1e-2))\n",
    "])\n",
    "\n",
    "# texts : the features of all samples. A list of texts of all positive and negative texts\n",
    "# y : the target. 1d array of 1 and 0 according to the criticism\n",
    "# clf.fit(texts, y)\n",
    "# print(np.mean(clf.predict(texts) == y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['good good', 'bad really bad', 'not bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# HAA - Test NB classification using CountVectorizer\n",
    "#       => 0.83\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.75, ngram_range=(1, 2),\n",
    "                             analyzer='word', stop_words=None)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8625  0.8     0.80625 0.8625  0.85625 0.8625  0.84375 0.80625 0.81875\n",
      " 0.7875 ]\n"
     ]
    }
   ],
   "source": [
    "#  HAA \n",
    "scores = cross_val_score(clf,        # steps to convert raw messages into models\n",
    "                         X_train,  # training data\n",
    "                         y_train,  # training labels\n",
    "                         cv=10,    # split data randomly into 10 parts: 9 for training, 1 for scoring\n",
    "                         scoring='accuracy',  # which scoring metric? string, callable or None, optional, default: None. https://scikit-learn.org/0.15/modules/model_evaluation.html\n",
    "                         n_jobs=-1,  # -1 => use all cores => faster. The number of CPUs to use to do the computation.\n",
    "                         )\n",
    "print (scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Could do you better with more data? Is the model complex too complex or too simple? Hint: Use a learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Can you do better using bigrams? Use parameter `ngram_range=(1, 2)` in CountVectorizer\n",
    "- Compare the learning curves using single words or bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the version number of a package\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.cross_validation import cross_val_score  # sklearn 0.22.1\n",
    "from sklearn.model_selection import cross_val_score  # sklearn 0.21.3\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.75, ngram_range=(1, 2),\n",
    "                             analyzer='word', stop_words=None)),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# TODO\n",
    "# HAA\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, train_size=0.8, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
