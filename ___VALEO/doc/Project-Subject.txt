For any questions regarding the challenge, you can contact the challenge provider 
at cda.challenge-valeo.mailbox@valeo.com

PROC_TRACEINFO,OP070_V_1_angle_value,OP090_SnapRingPeakForce_value,OP070_V_2_angle_value,OP120_Rodage_I_mesure_value,OP090_SnapRingFinalStroke_value,OP110_Vissage_M8_torque_value,OP100_Capuchon_insertion_mesure,OP120_Rodage_U_mesure_value,OP070_V_1_torque_value,OP090_StartLinePeakForce_value,OP110_Vissage_M8_angle_value,OP090_SnapRingMidPointForce_val,OP070_V_2_torque_value

------------------
Challenge context
------------------
﻿Valeo is a French global automotive supplier headquartered in France, listed on the Paris Stock Exchange (CAC-40 Index). 
It supplies a wide range of products to automakers and the aftermarket. 
The Group employs 113,600 people in 33 countries worldwide. 
It has 186 production plants, 59 R&D centers and 15 distribution platforms. 
Its strategy is focused on innovation and development in high-growth potential regions and emerging countries. 
Valeo ranked as France's leading patent filer from 2016 to 2018.

----------------
Challenge goals
----------------
﻿The goal of the challenge is to predict defect on starter motor production lines. 
During production samples assembly, different values (torques, angles ...) are measured on different mounting stations. 
At the end of the line, additional measures are performed on two test benches in order to isolate defects. 
As a result, samples are tagged ‘OK’, ‘KO’. 
We would like to design a model that could identify such defects before the test bench step.

On souhaite minimiser le taux de rejet sur le banc de test

------------------
Data description
------------------
- ﻿ID = PROC_TRACEINFO = it’s a unique code given to the product. Example : I-B-XA1207672-190701-00494.
XA1207672 is the reference.
190701 is the date: here 01st of July of year 2019.
00494 is the unique code given to the product, whatever it happens, the product will have this id number frozen forever.
This number is increased by 1 each time we process a new product, every 12s. 
So for example : I-B-XA1207672-190701-00495 is the next product.

- Inputs : Input features are measures collected on different assembly stations with the sensors or devices connected to Programmable Logic Controllers which are storing all of them to keep the full quality traceability. 
(Examples : OP070_V_1_angle_value, OP120_Rodage_I_value, etc…).

- Output : This is the result value of OP130 (test bench). 
Value 0 is assigned to OK samples (passed) and value 1 is assigned to KO samples (failed). 
This is the combined result of multiple electrical, acoustic and vibro-acoustic tests.

- The target is to find the best prediction : Output = f (inputs). 
The dataset contains 34515 training samples and 8001 test samples.

----------------------
Benchmark description
----------------------
﻿We expect a AUROC more than 0.675 which can easily be obtained with a basic Naive Bayes classifier. 
This AUROC value had been obtained using some techniques for unbalanced classes handling and not only with the challenge metric.
That's the main reason why our AUROC value of 0.675 is different from the benchmark value (0.5904).
 
- Public metric
roc_auc_score from scikit-learn : https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics

- Files
x_train : input data of the training set
y_train : output data of the training set
x_test  : input data of the testing set
random_submission_example : a random submission csv file example.

https://nlpparis.wordpress.com/
------
LINKS
------
GOOGLE ON: data science techniques for imbalanced classes handling
/* URL Peprocessing - Missing values */
https://towardsdatascience.com/4-tips-for-advanced-feature-engineering-and-preprocessing-ec11575c09ea
https://towardsdatascience.com/preprocessing-regression-imputation-of-missing-continuous-values-f612179bafb4
https://machinelearningmastery.com/handle-missing-data-python/



/* URL imbalanced data */
https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html
https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb
https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18
https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28
https://towardsdatascience.com/imbalanced-data-in-classification-general-solution-case-study-169f2e18b017
https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/
https://opendatascience.com/strategies-for-addressing-class-imbalance/   It seems that "Random Undersampling" performed the better
https://arxiv.org/pdf/1106.1813.pdf     SMOTE description
https://arxiv.org/pdf/1909.00169v3.pdf  Imbalance Problems in Object Detection
https://arxiv.org/pdf/2002.04592.pdf    Imbalanced classification: an objective-oriented review
https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/
https://www.researchgate.net/publication/318308371_Medical_imbalanced_data_classification

Notebook Unbalanced data:
https://github.com/njermain/RandomForestClassifier_ChubMackerel
https://github.com/njermain/RandomForestClassifier_ChubMackerel/blob/master/Imbalanced_classes.py

/* Transforming skewed data for ML */
https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0


- Discover how to systematically get good results for imbalanced classification:
Step-By-Step Framework for Imbalanced Classification Projects
https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/  transform data using log10 + use model robust toward skewness (eg. Trees algorithm)

- Discover how to work through a credit card fraud prediction project step-by-step:
Imbalanced Classification with the Fraudulent Credit Card Transactions Dataset
https://machinelearningmastery.com/imbalanced-classification-with-the-fraudulent-credit-card-transactions-dataset/

- Discover how to identify glass type based on the chemical composition:
Imbalanced Multiclass Classification with the Glass Identification Dataset
https://machinelearningmastery.com/imbalanced-multiclass-classification-with-the-glass-identification-dataset/


/* Guillaume Le Maitre */
https://github.com/scikit-learn-contrib/imbalanced-learn  					// GLM  Guillaume Le Maitre
https://imbalanced-learn.readthedocs.io/en/stable/auto_examples/index.html  // GLM  Guillaume Le Maitre

/* WEKA */
https://www.youtube.com/watch?v=j97h_-b0gvw   //Weka Tutorial 28: ROC Curves and AUC (Model Evaluation)
https://waikato.github.io/weka-wiki/downloading_weka/#windows  // java -jar weka.jar

/* scikit learn */
https://scikit-learn.org/stable/developers/utilities.html#validation-tools   // Utilities for Developers
https://scikit-learn.org/stable/user_guide.html   // User Guide
https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics  // API reference

/* EDA - Visual Data Exploration */
https://towardsdatascience.com/understanding-the-normal-distribution-with-python-e70bb855b027
https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4

/* Pandas */
https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-6fcd0170be9c
https://towardsdatascience.com/10-python-pandas-tricks-that-make-your-work-more-efficient-2e8e483808ba


/* PipeLine and Preprocessing */
https://project.inria.fr/dirtydata/
https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65
https://towardsdatascience.com/navigating-the-hell-of-nans-in-python-71b12558895b


/* Deep Learning that may be applied for the project */
https://towardsdatascience.com/deeplearning-with-tabular-data-data-processing-cd2e73908257

/* Deep learning - Generally*/
https://medium.com/@ODSC/5-essential-neural-network-algorithms-9336093fdf56
https://opendatascience.com/essential-neural-network-algorithms/
https://medium.com/@keonyonglee/bread-and-butter-from-deep-learning-by-andrew-ng-course-1-neural-networks-and-deep-learning-41563b8fc5d8


/* Autre liens interessant */
https://www.imo.universite-paris-saclay.fr/~goude/Materials/ProjetMLF/time_series.html


/* AUC & ROC*/
GOOGLE on : auroc vs auc
https://www.google.com/search?q=auroc+vs+auc&rlz=1C1GCEU_frFR867FR867&oq=auroc+vs+auc&aqs=chrome..69i57j0.32313j0j7&sourceid=chrome&ie=UTF-8

GOOGLE on : auroc
https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/
https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it

---------
Remarks:
---------
1/ Classification with Imbalanced Data Sets :
   Such a situation poses challenges for typical classifiers such as decision tree induction
   systems that are designed to optimize overall accuracy without taking into account the
   relative distribution of each class.
   As a result, these classifiers tend to ignore small classes while concentrating on classifying the large ones accurately.
 
2/ In inbalanced Datasets:
   Negative Exemple are around 99% (the OK)
   Positive Exemple are around 1%  (the KO)
   
3/ Imbalanced evaluation based on: Precision / Recall / F1
   A ROC curve displays a relation between sensitivity and and specificity for a given classifier
   (binary problems, parameterized classifier or a score classification)
   It is a two-dimensional graph to depicts trade-offs between benefits (true positives) 
   and costs (false positives)

-----------------
 Exemple de TP
-----------------
- http://localhost:8888/notebooks/Training/W4-1/docs_classification/04-text_classification_project-CORRECTED.ipynb 
  Graphe de Regularization C vs Accuracy  
- http://localhost:8888/notebooks/Training/W3-1/dssp_12_2019-master/04_trees/01_split.ipynb
  Decision Tree
  + Instead of trying to random split, we can use a greedy strategy by iteratively trying all possible value and use the one maximum the information gain.
- http://localhost:8888/tree/Training/W3-1/dssp_12_2019-master/05_advanced_sklearn_usage

  http://localhost:8888/notebooks/Training/W3-1/dssp_12_2019-master/05_advanced_sklearn_usage/03_mixed_type_preprocessing.ipynb 
  Complex machine-learning pipeline /  W9
  
  http://localhost:8888/notebooks/Training/W3-1/dssp_12_2019-master/05_advanced_sklearn_usage/04_parameters_search.ipynb
  Hyper-parameter tuning / W9
  
  http://localhost:8888/notebooks/Training/W3-1/dssp_12_2019-master/05_advanced_sklearn_usage/05_model_complexity_gridsearchcv.ipynb
  Parameter selection, Validation, and Testing  
  
  http://localhost:8888/notebooks/Training/W3-1/dssp_12_2019-master/05_advanced_sklearn_usage/06_learning_curves.ipynb
  Analysing model capacity
  
  
  
  
------
TODOs
------   
- Confusion Matrix 
- SSTic.pdf : p18 Classical evaluation(such Precision) doesn’t take
  into account the Negative Rate, which is very important in imbalanced problems 
- 3 lignes pour lesquels le min des features "OP090_* / 4 features" est égal à Zéro
  => Probablement mauvaise valeurs (noise) => Utiliser Imputer pour leur attribuer une valeur

------------------------------
 Notes Utiles pour le rapport:  
------------------------------ 
--> Suggestion de trame du rapport:
1. Look at the big picture.
2. Get the data.
3. Discover and visualize the data to gain insights.
4. Prepare the data for Machine Learning algorithms.
5. Select a model and train it.
6. Fine-tune your model.
7. Present your solution.
8. Launch, monitor, and maintain your system

https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb
https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4
https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62

- Correlation:
https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/
https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/linear-nonlinear-and-monotonic-relationships/    
    
- Missing Values:
https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779	

--> Phrase Clés:
- Your model should learn from this data and be able to predict the median housing
price in any district, given all the other metrics...... Voir page 37-Géron
- Voir page 44-Géron
The higher the norm index, the more it focuses on large values and neglects small ones. 
=> It gives a relatively high weight to large errors
This is why the RMSE is more sensitive to outliers than the MAE. 

But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
very well and is generally preferred.
- Sampling strategy p53-Géron


--> Source: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18
- Confusion Matrix: a table showing correct predictions and types of incorrect predictions.
- Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value.
  It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.
- Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity
  or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.

PROC_TRACEINFO,OP070_V_1_angle_value,OP090_SnapRingPeakForce_value,OP070_V_2_angle_value,OP120_Rodage_I_mesure_value,OP090_SnapRingFinalStroke_value,OP110_Vissage_M8_torque_value,OP100_Capuchon_insertion_mesure,OP120_Rodage_U_mesure_value,OP070_V_1_torque_value,OP090_StartLinePeakForce_value,OP110_Vissage_M8_angle_value,OP090_SnapRingMidPointForce_val,OP070_V_2_torque_value  